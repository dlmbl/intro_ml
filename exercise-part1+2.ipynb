{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning  \n",
    "# Exercise Part 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWC5tKyP3x1e"
   },
   "source": [
    "Written by Morgan Schwartz and David Van Valen.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Some code cells will be marked with \n",
    "```\n",
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "```\n",
    "\n",
    "This indicates that you are being asked to write a piece of code to complete the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Linear Classifier\n",
    "\n",
    "To illustrate the workflow for training a deep learning model in a supervised manner, this notebook will walk you through the simple case of training a linear classifier to recognize images various stages of the cell cycle. While deep learning might seem intimidating, don't worry. Its conceptual underpinnings are rooted in linear algebra and calculus - if you can perform matrix multiplication and take derivatives you can understand what is happening in a deep learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DwKG_Gi3x1f"
   },
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import skimage\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ2jTjsd3x1g"
   },
   "source": [
    "## The supervised machine learning workflow\n",
    "Recall from class the conceptual workflow for a supervised machine learning project. \n",
    "- First, we create a <em>training dataset</em>, a paired collection of raw data and labels where the labels contain information about the \"insight\" we wish to extract from the raw data. \n",
    "- Once we have training data, we can then use it to train a <em>model</em>. The model is a mathematical black box - it takes in data and transforms it into an output. The model has some parameters that we can adjust to change how it performs this mapping. \n",
    "- Adjusting these parameters to produce outputs that we want is called training the model. To do this we need two things. First, we need a notion of what we want the output to look like. This notion is captured by a <em>loss function</em>, which compares model outputs and labels and produces a score telling us if the model did a \"good\" job or not on our given task. By convention, low values of the loss function's output (e.g. the loss) correspond to good performance and high values to bad performance. We also need an <em>optimization algorithm</em>, which is a set of rules for how to adjust the model parameters to reduce the loss\n",
    "- Using the training data, loss function, and optimization algorithm, we can then train the model \n",
    "- Once the model is trained, we need to evaluate its performance to see how well it performs and what kinds of mistakes it makes. We can also perform this kind of monitoring during training (this is actually a standard practice).\n",
    "\n",
    "Because this workflow defines the lifecycle of most machine learning projects, this notebook is structured to go over each of these steps while constructing a linear classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbIDLvJ23x1g"
   },
   "source": [
    "## Create training data\n",
    "The starting point of every machine learning project is data. Today we are going to look at a collection of images of Jurkat cells published in the Broad Bioimage Collection ([BBBC048](https://bbbc.broadinstitute.org/BBBC048)). The cells were fixed and stained with PI (propidium iodide) to quantify DNA content and a MPM2 (mitotic protein monoclonal #2) antibody to identify mitotic cells.\n",
    "\n",
    "During the initial setup of this exercise, we downloaded the data and unzipped the relevant files using the script `data-download.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/CellCycle'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above should generate the following output. Order does not matter, but you should see the same files and folders. If you see something different, please check that the `data-download.sh` script ran correctly.\n",
    "```\n",
    "['img.lst~',\n",
    " 'Anaphase',\n",
    " 'Prophase',\n",
    " 'img.lst',\n",
    " 'S',\n",
    " '66.lst~',\n",
    " 'G1',\n",
    " 'Metaphase',\n",
    " 'G2',\n",
    " 'Telophase']\n",
    " ```\n",
    " \n",
    "The metadata for each file is stored in `img.lst` so we will first load this information to inform how we load the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe with sample info\n",
    "df = pd.read_csv(os.path.join(data_dir, 'img.lst'), sep='\\t', header=None)\n",
    "df = df.rename(columns={1: 'class', 2: 'filepath'})\n",
    "df['channel'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[1].str.slice(2,3)\n",
    "df['id'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each `id` there are three images. One for each of the channels: phase, PI and MPM2. We will load each image and stack it into an array of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data stack\n",
    "ims = []\n",
    "ys = []\n",
    "for i, g in df.groupby('id'):\n",
    "    im = []\n",
    "    for _, r in g.iterrows():\n",
    "        im.append(iio.imread(os.path.join(data_dir, r['filepath'])))\n",
    "    ims.append(np.stack(im, axis=-1))\n",
    "    ys.append(r['class'])\n",
    "    \n",
    "X_data = np.stack(ims)\n",
    "y_data = np.stack(ys)\n",
    "print('X shape:', X_data.shape)\n",
    "print('y shape:', y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG4mmJ173x1h"
   },
   "source": [
    "In the previous cell, you probably observed that there are 4 dimensions rather than the 3 you might have been expecting. This is because while each image is (66, 66, 3), the full dataset has many images. The different images are stacked along the first dimension. The full size of the training images is (# images, 66, 66, 3) - the first dimension is often called the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.1\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Use matplotlib (plt.imshow) to visualize several images randomly drawn from the dataset\n",
    "# There are 7 classes in the dataset. Make sure to look at an example or each class\n",
    "# For clarity, split each 3 channel image into one plot per channel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Class Balance\n",
    "\n",
    "Let's check the balance of classes in this dataset. There are at least two ways you could do this. One would be to use matplotlib to create a histogram. The other would be to count the number of items in each class using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.2\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Add your code to check class balances here\n",
    "# You should end up with a count of number of items in each of the 7 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is highly inbalanced so we will want to correct the class balance before training. We will resample the data after splitting it below. We can improve the balance of samples by upsampling the minority class to match the number of samples in the majority class using [`sklearn.utils.resample`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract classes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we are going to restrict the dataset to two classes `[4, 5]` and a single channel `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(X_data, y_data, classes):\n",
    "    \"\"\"For a given dataset of categorical labels, this data\n",
    "    selects the datapoints associated with the classes of interest\n",
    "    and returns them stacked in a new array\n",
    "    \n",
    "    Args:\n",
    "        X_data (np.array): Array of x data\n",
    "        y_data (np.array): Array of categorical y data\n",
    "        classes (list): List of categorical classes to select\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X data after extracting the classes of interest \n",
    "        np.array: y data after selecting the classes of interest\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for c in classes:\n",
    "        # Identify the indicies of the relevant class\n",
    "        idx = y_data == c\n",
    "        # Select the X and y data accordingly\n",
    "        X.append(X_data[idx, ..., 0:1])\n",
    "        y.append(y_data[idx])\n",
    "\n",
    "    # Restack the arrays\n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_classes(X_data, y_data, [4, 5])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reassign classes 4 and 5 to 0 and 1 for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 4] = 0\n",
    "y[y == 5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D_YhVir3x1i"
   },
   "source": [
    "For this exercise, we will want to flatten the training data into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the original width so that we can use this for reshaping later\n",
    "image_width = X_data[0].shape[0]\n",
    "image_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3MJPnh-3x1j",
    "outputId": "c280504b-94c4-4888-dd02-875c789dee9f"
   },
   "outputs": [],
   "source": [
    "# Flatten the images 1d vectors\n",
    "X = np.reshape(X, (-1, image_width * image_width, 1))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2yrGjOL3x1j"
   },
   "source": [
    "### Split the training dataset into training and testing datasets\n",
    "How do we know how well our model is doing? A common practice to evaluate models is to evaluate them on splits of the original training dataset. Splitting the data is important, because we want to see how models perform on data that was not used to train them.\n",
    "- The <em>training</em> dataset used to train the model\n",
    "- A held out <em>testing</em> dataset used to evaluate the final trained version of the model\n",
    "While there is no hard and fast rule, 80%/20% splits are a reasonable starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLQUiSoj3x1j"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing splits\n",
    "seed = 10\n",
    "train_size = 0.8\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    train_size=train_size, \n",
    "    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.3\n",
    "\n",
    "Write a function to increase the number of samples in the minority class to match the number of samples in the majority class. Your function should take in the X and y arrays and return X and y arrays after upsampling. Make sure to shuffle the data before returning it to restore a randomized order.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "def balance_classes(X, y, minority_id):\n",
    "    \"\"\"For a given minority class id, upsample the minority class\n",
    "    to match the number of samples in the majority class\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Array of raw data\n",
    "        y (np.array): Array of class labels\n",
    "        minority_id (int): Integer of the minority class to be upsampled\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X\n",
    "        np.array: y\n",
    "    \"\"\"\n",
    "    # Split the X and y arrays into sub arrays containing \n",
    "    # 1) only the minority samples and 2) the remaining samples\n",
    "    \n",
    "    # Use sklearn.utils.resample to samples from the minority sample array \n",
    "    # to match the number of samples in the second array\n",
    "    \n",
    "    # Concatenate the upsampled array 1 with the remaining array 2\n",
    "    \n",
    "    # Shuffle the concatenated arrays\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = balance_classes(X_train, y_train, 1)\n",
    "X_test, y_test = balance_classes(X_test,  y_test, 1)\n",
    "print('Train shape:', X_train.shape,  y_train.shape)\n",
    "print('Test shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fhlPKpe3x1j"
   },
   "source": [
    "## The linear classifier\n",
    "The linear classifier produces class scores that are a linear function of the pixel values. Mathematically, this can be written as $\\vec{y} = W \\vec{x}$, where $\\vec{y}$ is the vector of class scores, $W$ is a matrix of weights and $\\vec{x}$ is the image vector. The shape of the weights matrix is determined by the number of classes and the length of the image vector. In this case $W$ is 2 by 4356. Our learning task is to find a set of weights that maximize our performance on our classification task. We will solve this task by doing the following steps\n",
    "- Randomly initializing a set of weights\n",
    "- Defining a loss function that measures our performance on the classification task\n",
    "- Use stochastic gradient descent to find \"optimal\" weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0LQ99KR3x1j",
    "tags": []
   },
   "source": [
    "### Create the matrix of weights\n",
    "Properly initializing weights is essential for getting deep learning methods to work correctly. We are going to start with weights initizalized with zeros, but will invesigate other methods later in the exercise.\n",
    "\n",
    "Lets create the linear classifier using object oriented programming, which will help with organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self, image_size=image_width * image_width, n_classes=2):\n",
    "        self.image_size = image_size\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):  \n",
    "        self.W = np.zeros((self.n_classes, self.image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yeA5geI3x1k"
   },
   "source": [
    "### Apply the softmax transform to complete the model outputs\n",
    "Our LinearClassifier class needs a method to perform predictions - which in our case is performing matrix multiplication and then applying the softmax transform.  The softmax functions transforms a vector of arbitrary real numbers and turns it into probabilities that we can use for our final prediction.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*gctBX5YHUUpBEK3MWD6r3Q.png)\n",
    "(*Image by [Thomas Kurbiel](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)*)\n",
    "\n",
    "There is an excellent [derivation of the softmax function](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1) available on Towards Data Science if you are interested in the details of the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YadGdARMgRJ0",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Complete the predict function below to predict a label y from an input X\n",
    "# Note that our predict function is going to start by removing the channel dimension from the X data.\n",
    "# Pay careful attention to the shape of your data at each step\n",
    "\n",
    "def predict(self, X, epsilon=1e-5):\n",
    "    X = X[..., 0]\n",
    "    pass\n",
    "    #y = # matrix multiplication\n",
    "\n",
    "    #y = # Apply softmax\n",
    "    return y\n",
    "\n",
    "# Assign methods to class\n",
    "setattr(LinearClassifier, 'predict', predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv4Rc_xS3x1l"
   },
   "source": [
    "## Stochastic gradient descent\n",
    "To train this model, we will use stochastic gradient descent. In its simplest version, this algorithm consists of the following steps:\n",
    "- Select several images from the training dataset at random\n",
    "- Compute the gradient of the loss function with respect to the weights, given the selected images\n",
    "- Update the weights using the update rule $\\Delta W_{ij} \\rightarrow \\Delta W_{ij} - lr\\frac{\\partial loss}{\\partial W_{ij}}$\n",
    "\n",
    "Recall that the origin of this update rule is from multivariable calculus - the gradient tells us the direction in which the loss function increases the most. So to minimize the loss function we move in the opposite direction of the gradient.\n",
    "\n",
    "Also recall from the course notes that for this problem we can compute the gradient analytically. The gradient is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial loss}{\\partial W_{ij}} = \\left(p_i - 1(i \\mbox{ is correct}) \\right)x_j,\n",
    "\\end{equation}\n",
    "where $1$ is an indicator function that is 1 if the statement inside the parentheses is true and 0 if it is false.\n",
    "\n",
    "A complete derivation of $\\frac{\\partial loss}{\\partial W_{ij}}$ is included in the Towards Data Science [article](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1) recommended above if you are interested in the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXHGyfz33x1l"
   },
   "outputs": [],
   "source": [
    "def grad(self, X, y):\n",
    "    # Get class probabilities\n",
    "    p = self.predict(X)\n",
    "    \n",
    "    # Compute class 0 gradients\n",
    "    temp_0 = np.expand_dims(p[...,0] - (1-y), axis=-1)\n",
    "    grad_0 = temp_0 * X[...,0]\n",
    "\n",
    "    # Compute class 1 gradients\n",
    "    temp_1 = np.expand_dims(p[...,1] - y, axis=-1)\n",
    "    grad_1 =  temp_1 * X[...,0]\n",
    "    \n",
    "    gradient = np.stack([grad_0, grad_1], axis=1)\n",
    "    \n",
    "    return gradient\n",
    "    \n",
    "def loss(self, X, y_true):\n",
    "    y_pred = self.predict(X)\n",
    "    \n",
    "    # Convert y_true to one hot\n",
    "    y_true = np.stack([y_true, 1-y_true], axis=-1)\n",
    "    loss = np.mean(-y_true * np.log(y_pred))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def fit(self, X_train, y_train, n_epochs, batch_size=1, learning_rate=1e-5):\n",
    "    loss_list = []\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = int(np.floor(X_train.shape[0] / batch_size))\n",
    "        \n",
    "        # Generate random index\n",
    "        index = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for batch in range(n_batches):\n",
    "            beg = batch*batch_size\n",
    "            end = (batch+1)*batch_size if (batch+1)*batch_size < X_train.shape[0] else -1\n",
    "            X_batch = X_train[beg:end]\n",
    "            y_batch = y_train[beg:end]\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = self.loss(X_batch, y_batch)\n",
    "            loss_list.append(loss)\n",
    "            \n",
    "            # Compute the gradient\n",
    "            gradient = self.grad(X_batch, y_batch)\n",
    "            \n",
    "            # Compute the mean gradient over all the example images\n",
    "            gradient = np.mean(gradient, axis=0, keepdims=False)\n",
    "\n",
    "            # Update the weights\n",
    "            self.W -= learning_rate * gradient\n",
    "            \n",
    "    return loss_list\n",
    "\n",
    "# Assign methods to class\n",
    "setattr(LinearClassifier, 'grad', grad)\n",
    "setattr(LinearClassifier, 'loss', loss)\n",
    "setattr(LinearClassifier, 'fit', fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, let's take a brief moment to check what the untrained model predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4tgJymU33x1l",
    "outputId": "736e325f-56eb-4bf7-9caf-bc6591c2a448"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20,10))\n",
    "for i, j in enumerate(np.random.randint(X_test.shape[0], size=(8,))):\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = X_test[j,...]\n",
    "    \n",
    "    # Reshape flattened vector to image\n",
    "    X_reshape = np.reshape(X_sample, (66, 66))\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred = lc.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(y_test[j]) +', Prediction ' + str(np.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to inspecting the results of individual predictions, we can also look at summary statistics that capture model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(y_true, y_pred):\n",
    "    \"\"\"Calculates recall, precision, f1 and a confusion matrix for sample predictions\n",
    "    \n",
    "    Args:\n",
    "        y_true (list): List of integers of true class values\n",
    "        y_pred (list): List of integers of predicted class value\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary with keys `recall`, `precision`, `f1`, and `cm`\n",
    "    \n",
    "    \"\"\"\n",
    "    _round = lambda x: round(x, 3)\n",
    "    \n",
    "    metrics = {\n",
    "        'recall': _round(sklearn.metrics.recall_score(y_true, y_pred)),\n",
    "        'precision': _round(sklearn.metrics.precision_score(y_true, y_pred)),\n",
    "        'f1': _round(sklearn.metrics.f1_score(y_true, y_pred)),\n",
    "        'cm': sklearn.metrics.confusion_matrix(y_true, y_pred, normalize=None),\n",
    "        'cm_norm': sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.5\n",
    "For each of the 4 metrics above, describe in your own words what this metric tells you about model performance.\n",
    "\n",
    "- Recall\n",
    "- Precision\n",
    "- F1\n",
    "- Confusion Matrix\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, name, ax=None):\n",
    "    \"\"\"Plots a confusion matrix with summary statistics listed above the plot\n",
    "    \n",
    "    The annotations on the confusion matrix are the total counts while\n",
    "    the colormap represents those counts normalized to the total true items\n",
    "    in that class.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Dictionary output of `benchmark_performance`\n",
    "        name (str): Title for the plot\n",
    "        ax (optional, matplotlib subplot): Subplot axis to plot onto. \n",
    "            If not provided, a new plot is created\n",
    "        classes (optional, list): A list of the classes to label the X and y \n",
    "            axes. Defaults to [0, 1] for a two class problem.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cb = ax.imshow(metrics['cm_norm'], cmap='Greens', vmin=0, vmax=1)\n",
    "    \n",
    "    classes = np.arange(metrics['cm'].shape[0])\n",
    "    plt.xticks(range(len(classes)), classes)\n",
    "    plt.yticks(range(len(classes)), classes)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            color='green' if metrics['cm_norm'][i,j] < 0.5 else 'white'\n",
    "            ax.annotate('{}'.format(metrics['cm'][i,j]), (j, i),\n",
    "                        color=color, va='center', ha='center')\n",
    "\n",
    "    _ = plt.colorbar(cb, ax=ax)\n",
    "    _ = ax.set_title(\n",
    "            '{}\\n'\\\n",
    "            'Recall: {}\\n'\\\n",
    "            'Precision: {}\\n'\\\n",
    "            'F1 Score: {}\\n'\\\n",
    "            ''.format(name, metrics['recall'], metrics['precision'], metrics['f1'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and metrics for training data\n",
    "y_pred = lc.predict(X_train)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "metrics = benchmark_performance(y_train, y_pred)\n",
    "\n",
    "plot_metrics(metrics, 'Untrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKRfu3J3x1l",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 1.6\n",
    "What do you notice about the initial results of the model? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model and compare the performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss = lc.fit(X_train, y_train, n_epochs=32, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh-GWNb-3x1m"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T_wsv4zW3x1m",
    "outputId": "1d5928f9-1acb-43ec-8c58-ad756e538171"
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20,10))\n",
    "for i, j in enumerate(np.random.randint(X_test.shape[0], size=(8,))):\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = X_test[j]\n",
    "    \n",
    "    # Reshape flattened vector to image\n",
    "    X_reshape = np.reshape(X_sample, (66, 66))\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred = lc.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(y_test[j]) +', Prediction ' + str(np.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and metrics for training data\n",
    "y_pred = lc.predict(X_train)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "train_metrics = benchmark_performance(y_train, y_pred)\n",
    "\n",
    "# Generate predictions and metrics for test data\n",
    "y_pred = lc.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "test_metrics = benchmark_performance(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "plot_metrics(train_metrics, 'Training', ax[0])\n",
    "plot_metrics(test_metrics, 'Testing', ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#### Task 1.7\n",
    "\n",
    "What do you notice about the results after training the model?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXwiVLR4X_WY"
   },
   "source": [
    "# Part 2: Implementing a linear classifier with TensorFlow\n",
    "In this section, we will define our machine learning models using TensorFlow. These models are composed of layers - each layer specifies a mathematical operation that is applied to its input. The nice thing about TensorFlow is that almost all of the machinery required for stochastic gradient descent is taken care of for us.\n",
    "- Specify trainable variables? Check.\n",
    "- Initialize trainable variables with random values? Check.\n",
    "- Compute the layer outputs? Check.\n",
    "- Compute gradients using backpropagation? Check.\n",
    "- Perform all of the computations on GPUs to speed up training and inference? Check.\n",
    "All of the above (and more) are taken care of for us by TensorFlow - writing models often requires little math (although one practice that I encourage is keeping track of the input and output dimensions for each layer).\n",
    "\n",
    "To define a linear classifier, we will use a module in TensorFlow called Keras. Keras simple APIs for specifying models. In Keras, there are two different APIs you can use:\n",
    "- [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) - If your model is composed of a linear sequence of steps, this is the easier API to use.\n",
    "- [Functional API](https://www.tensorflow.org/guide/keras/functional) - If your model is more complicated, this API provides more flexibility. If you're using the functional API, consider using a class with methods to write submodels.\n",
    "The TensorFlow documentation provides additional details about how to use each of these two APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pkm3aspYX_WZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIHbR3PwX_WW"
   },
   "source": [
    "## Create dataset object\n",
    "TensorFlow uses Dataset objects to feed data into the training pipeline. In this section, we will write a function to assemble Dataset objects after splitting the data and balancing classes as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(X, y, batch_size=1, seed=1, train_size=0.8):\n",
    "    # Create train/test splits\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, \n",
    "        train_size=train_size, \n",
    "        random_state=seed)\n",
    "    \n",
    "    # Balance classes in each split\n",
    "    X_train, y_train = balance_classes(X_train, y_train, 1)\n",
    "    X_test, y_test = balance_classes(X_test, y_test, 1)\n",
    "    \n",
    "    # Convert y data to categorical\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(256).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Tq-Hg_mX_WX"
   },
   "outputs": [],
   "source": [
    "X, y = extract_classes(X_data, y_data, [4, 5])\n",
    "y[y == 4] = 0\n",
    "y[y == 5] = 1\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "    dataset = build_dataset(X, y, batch_size=16, seed=seed, train_size=train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbDlQmgBX_WY",
    "outputId": "c4e64f0d-686c-4210-ec11-b74cd391eebb"
   },
   "outputs": [],
   "source": [
    "# Check the dataset builder\n",
    "it = dataset['train'].as_numpy_iterator()\n",
    "X_temp, y_temp = it.next()\n",
    "print(X_temp.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20,10))\n",
    "for i in range(4):\n",
    "    axes.flatten()[i].imshow(X_temp[i,...,:], cmap='Greys')\n",
    "    axes.flatten()[i].set_title('Label ' + str(y_temp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-jix6tiX_WZ",
    "outputId": "a02646e0-7172-47c4-d57c-bdfb75e98ff9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the linear classifier\n",
    "def create_linear_classifier():\n",
    "    inputs = Input((X.shape[1], X.shape[2], 1),\n",
    "                   name='linear_classifier_input')\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(2)(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "linear_classifier = create_linear_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaMQbitnX_Wa"
   },
   "source": [
    "## Specify training parameters \n",
    "In this section, we will specify how we want to train the neural network. We will need to specify three things:\n",
    "- The loss function: Because we are training a model for classification, we will use the categorical crossentropy\n",
    "- The training algorithm: There are many flavors of stochastic gradient descent - for this problem, we will use a variant called Adam\n",
    "- The training parameters: The training algorithm needs parameters like the learning rate, number of epochs, number of steps per epoch, etc. to be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veiaElEQX_Wb"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy() \n",
    "\n",
    "# Define the training algorithm\n",
    "linear_optimizer = tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define training parameters\n",
    "n_epochs=32\n",
    "\n",
    "# Define callbacks\n",
    "linear_model_path = 'linear'\n",
    "\n",
    "# Define metrics\n",
    "recall_0 = tf.keras.metrics.Recall(class_id=0)\n",
    "recall_1 = tf.keras.metrics.Recall(class_id=1)\n",
    "\n",
    "precision_0 = tf.keras.metrics.Precision(class_id=0)\n",
    "precision_1 = tf.keras.metrics.Precision(class_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models\n",
    "linear_classifier.compile(optimizer=linear_optimizer, \n",
    "                          loss=loss_function, \n",
    "                          metrics = [recall_0, recall_1, precision_0, precision_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzRK8b8FX_Wb"
   },
   "source": [
    "## Train the model\n",
    "With the dataset, model, and training parameters defined, it is straightforward to train a model. Keras Model objects have a fit method that takes in the training parameters and executes the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rsv5dmnX_Wc",
    "outputId": "005cec17-54f5-4ef8-9e34-979732d43443"
   },
   "outputs": [],
   "source": [
    "# Train the linear classifier\n",
    "linear_classifier.fit(dataset['train'],\n",
    "                      epochs=n_epochs,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbiwMkypX_Wc"
   },
   "source": [
    "## Benchmark the model\n",
    "In this section, we will benchmark each model to assess the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFhuK1YKX_Wd",
    "outputId": "f60e1843-02d5-4600-8889-db8cd7f3055e"
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "it = dataset['test'].as_numpy_iterator()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20,10))\n",
    "for i in range(8):\n",
    "    X_test, y_test = it.next()\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = X_test[[i],...]\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred_linear = linear_classifier.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_sample[0], cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(np.argmax(y_test[i])) +', Prediction ' + str(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "test_list = list(dataset['test'].as_numpy_iterator())\n",
    "X_test = np.concatenate([item[0] for item in test_list], axis=0)\n",
    "y_test = np.concatenate([item[1] for item in test_list], axis=0)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Compute linear classifier metrics\n",
    "y_pred = linear_classifier.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "metrics = benchmark_performance(y_test, y_pred)\n",
    "\n",
    "plot_metrics(metrics, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Task 2.1\n",
    "\n",
    "Compare and contrast the performance of the first linear classifier that we trained to the version trained in tensorflow.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h2>Checkpoint 1</h2>\n",
    "    \n",
    "Please üëç the \"Checkpoint 1\" slack thread when you reach this checkpoint.\n",
    "\n",
    "We will come back together as a group to discuss the exercise up to this point before moving on to the next notebook. If you have some extra time, try modifying our tensorflow model to work on 3 or more classes instead of just 2. At minimum, you will need to make the folloing modifications:\n",
    "- Create a new copy of the dataset that is balanced across all classes\n",
    "- Modify `create_linear_classifier` to work on 3 classes\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:01_intro_ml]",
   "language": "python",
   "name": "conda-env-01_intro_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
